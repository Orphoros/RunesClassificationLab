{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Feature extraction\n",
    "\n",
    "In this iteration, (iter 1), we want to use machine learning models. For a better classification with machine learning models, we extract more features, and evaluate the extracted features to see what are the best for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adding the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage as si\n",
    "from skimage import io, measure\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load in all images\n",
    "\n",
    "All images are loaded in into an array as a tuple of (image, label). This way we know what image we are dealing with. We also lowercase all the labels as \"Tyr\" is in uppercase, while everything else is in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1472\n",
      "All labels: {'tyr', 'wealth', 'serpent', 'oak', 'spear', 'gift', 'elk-sedge', 'ash', 'bow', 'joy', 'need', 'sun'}\n"
     ]
    }
   ],
   "source": [
    "directory = \"../dataset-images/\" # Path to the dataset\n",
    "images = [] # List of images\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is an image\n",
    "    if filename.endswith(\".png\"):\n",
    "        label = filename.split(\"_\")[0].lower() # Get the label\n",
    "        # Label and load the image\n",
    "        img = io.imread(os.path.join(directory, filename))\n",
    "        images.append((img, label))\n",
    "\n",
    "# Print the number of images\n",
    "print(\"Number of images: \" + str(len(images)))\n",
    "print(\"All labels: \" + str(set([label for _, label in images])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature extraction\n",
    "\n",
    "In this iteration, we want to extract as many features as possible. We extract 2 types of features:\n",
    "\n",
    "1. Region prop features where the returned datatype is a positive (for selecting K best features with `chi2` later on) number and not a float (so we can store it in a CSV)\n",
    "2. Smart custom features\n",
    "\n",
    "We created a collection of 2 smart features: `vertical and horizontal symmetry` and `feature density for 9 regions of an image`\n",
    "\n",
    "Some runes are symmetric horizontally, some vertically, some not at all. This provides us a valuable new feature. With feature density, we count the amount of black pixels for each 9 regions (grid) of an image. This tells us in which region does most of the information (black pixel sum) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arrays_to_same_shape(arr1, arr2):\n",
    "    \"\"\"Make arrays the same same shape\n",
    "\n",
    "    If `arr1` or `arr2` are not of the same shape, this function will pad the smaller array to match the shape of the bigger one.\n",
    "    \"\"\"\n",
    "    # Get the shapes of the arrays\n",
    "    shape1 = np.shape(arr1)\n",
    "    shape2 = np.shape(arr2)\n",
    "\n",
    "    # If the shapes are not the same, pad the smaller array with zeros\n",
    "    if shape1 != shape2:\n",
    "        # Determine which array is smaller\n",
    "        if shape1[0] < shape2[0]:\n",
    "            # Pad arr1 with zeros\n",
    "            arr1 = np.pad(arr1, ((0, shape2[0] - shape1[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "        elif shape1[0] > shape2[0]:\n",
    "            # Pad arr2 with zeros\n",
    "            arr2 = np.pad(arr2, ((0, shape1[0] - shape2[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "        if shape1[1] < shape2[1]:\n",
    "            # Pad arr1 with zeros\n",
    "            arr1 = np.pad(arr1, ((0, 0), (0, shape2[1] - shape1[1])), 'constant', constant_values=(0))\n",
    "        elif shape1[1] > shape2[1]:\n",
    "            # Pad arr2 with zeros\n",
    "            arr2 = np.pad(arr2, ((0, 0), (0, shape1[1] - shape2[1])), 'constant', constant_values=(0))\n",
    "    \n",
    "    return arr1, arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "    \"\"\"Crop the image to where black pixels can be found\n",
    "\n",
    "    This function makes sure that each image is cropped down to the rune's dimension.\n",
    "    \"\"\"\n",
    "     # find the first black pixel from the left\n",
    "    left = 0\n",
    "    for i in range(img.shape[1]):\n",
    "        if np.any(img[:, i] == 0):\n",
    "            left = i\n",
    "            break\n",
    "    # find the first black pixel from the right\n",
    "    right = 0\n",
    "    for i in reversed(range(img.shape[1])):\n",
    "        if np.any(img[:, i] == 0):\n",
    "            right = i\n",
    "            break\n",
    "    # find the first black pixel from the top\n",
    "    top = 0\n",
    "    for i in range(img.shape[0]):\n",
    "        if np.any(img[i, :] == 0):\n",
    "            top = i\n",
    "            break\n",
    "    # find the first black pixel from the bottom\n",
    "    bottom = 0\n",
    "    for i in reversed(range(img.shape[0])):\n",
    "        if np.any(img[i, :] == 0):\n",
    "            bottom = i\n",
    "            break\n",
    "    # crop the image to the smallest rectangle containing the image\n",
    "    cropped_img = img[top:bottom+1, left:right+1]\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_v_symmetry_errors(img_symmetry):\n",
    "    \"\"\"Calculate the horizontal and vertical symmetry error rate of an image\n",
    "\n",
    "    This function returns a tuple of 2 floats: `(horizontal_sym_errors, vertical_sym_errors)`\n",
    "\n",
    "    Each returned number is a percentage of errors on how many pixels are not symmetric. 1 means the image not symmetric at all (100% errors)\n",
    "    and 0 means every pixel is symmetric (0% errors)\n",
    "    \"\"\"\n",
    "    middle_horizontal = img_symmetry.shape[0] // 2\n",
    "    middle_vertical = img_symmetry.shape[1] // 2\n",
    "    # check if the image is horizontally symmetric\n",
    "    top = img_symmetry[:middle_horizontal, :]\n",
    "    bottom = np.flip(img_symmetry[middle_horizontal:, :], axis=0)\n",
    "    # check if the shape is the same for top and bottom and if not, insert rows of zeros\n",
    "    top, bottom = pad_arrays_to_same_shape(top, bottom)\n",
    "    result = top ^ bottom\n",
    "    sum_result = np.sum(result) / (result.shape[0] * result.shape[1])\n",
    "    horizontal_sym_errors = sum_result\n",
    "    # check if the image is vertically symmetric\n",
    "    left = img_symmetry[:, :middle_vertical]\n",
    "    right = np.flip(img_symmetry[:, middle_vertical:], axis=1)\n",
    "    # check if the shape is the same for left and right and if not, insert columns of zeros\n",
    "    left, right = pad_arrays_to_same_shape(left, right)\n",
    "    result = left ^ right\n",
    "    sum_result = np.sum(result) / (result.shape[0] * result.shape[1])\n",
    "    vertical_sym_errors = sum_result\n",
    "    \n",
    "    return horizontal_sym_errors, vertical_sym_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_black_pixels(img, regions=1):\n",
    "    \"\"\"Count the amount of black pixels in an image\n",
    "\n",
    "    An optional `regions` may be set to count the black pixels for different regions (grids) of the original image\n",
    "\n",
    "    This function returns an array of each defined grid's (default 1) summed black pixels\n",
    "    \"\"\"\n",
    "    # split the image into a grids\n",
    "    img_split = np.array_split(img, regions)\n",
    "    # get plack pixels for each grid\n",
    "    black_pixels = []\n",
    "    for i in range(regions):\n",
    "        for j in range(regions):\n",
    "            black_pixels.append(np.sum(img_split[i][j] == 0))\n",
    "    return black_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new Pandas dataframe with all our extracted features combined with our smart features. To get the best results, we first crop the image, then resize the image to the original (128x128) size. This makes sure that the image takes up the whole space, and not just a small part of it (in case someone drew a rune in the corner of the image). We also add a 2 pixel border on top of our image. This is because the runes are stretched to the edges of the image, which would result in incorrect hole count (since the image touches the edges, resulting in more holes). This step helps us to get a more accurate hole count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               area   area_filled   area_convex  axis_major_length  \\\n",
      "count   1472.000000   1472.000000   1472.000000        1472.000000   \n",
      "mean    6107.643342   6470.124321  11039.891984         143.646528   \n",
      "std     1779.865675   2090.570961   1752.918338          16.273829   \n",
      "min     1469.000000   1469.000000   1808.000000         110.482240   \n",
      "25%     5013.250000   5085.000000   9668.750000         132.328105   \n",
      "50%     5990.000000   6143.500000  10519.000000         142.231762   \n",
      "75%     6853.500000   7304.500000  12155.250000         153.240164   \n",
      "max    15376.000000  15376.000000  15376.000000         202.711313   \n",
      "\n",
      "       axis_minor_length  eccentricity  equivalent_diameter_area       extent  \\\n",
      "count        1472.000000   1472.000000               1472.000000  1472.000000   \n",
      "mean          104.686584      0.639563                 87.404364     0.398024   \n",
      "std            14.929751      0.168373                 11.707400     0.115475   \n",
      "min            14.198591      0.000000                 43.247993     0.186134   \n",
      "25%            94.841866      0.556672                 79.894102     0.326775   \n",
      "50%           103.136358      0.670292                 87.331006     0.389958   \n",
      "75%           115.644205      0.754218                 93.413849     0.445955   \n",
      "max           146.320376      0.995135                139.919017     1.000000   \n",
      "\n",
      "       feret_diameter_max    perimeter  ...  v_sym_err_percent  pixelsum_tl  \\\n",
      "count         1472.000000  1472.000000  ...        1472.000000       1472.0   \n",
      "mean           151.881842   660.711596  ...           0.344802          0.0   \n",
      "std             15.619474   117.829284  ...           0.135186          0.0   \n",
      "min            120.208153   281.213203  ...           0.000000          0.0   \n",
      "25%            138.061580   557.151804  ...           0.248959          0.0   \n",
      "50%            152.775978   657.157900  ...           0.355879          0.0   \n",
      "75%            166.973052   715.155375  ...           0.450832          0.0   \n",
      "max            174.656806  1301.069155  ...           0.636446          0.0   \n",
      "\n",
      "       pixelsum_tm  pixelsum_tr  pixelsum_ml  pixelsum_mm  pixelsum_mr  \\\n",
      "count       1472.0  1472.000000  1472.000000  1472.000000  1472.000000   \n",
      "mean           0.0    23.104620    62.328125    62.101902    61.955842   \n",
      "std            0.0    16.056492    22.936268    22.927111    23.022321   \n",
      "min            0.0     4.000000    12.000000    14.000000    14.000000   \n",
      "25%            0.0    16.000000    46.000000    46.000000    45.000000   \n",
      "50%            0.0    19.000000    61.000000    60.000000    60.500000   \n",
      "75%            0.0    25.000000    77.000000    77.000000    78.000000   \n",
      "max            0.0   124.000000   124.000000   124.000000   124.000000   \n",
      "\n",
      "       pixelsum_bl  pixelsum_bm  pixelsum_br  \n",
      "count  1472.000000  1472.000000  1472.000000  \n",
      "mean     44.868207    43.950408    43.129755  \n",
      "std      27.094146    26.444835    25.931199  \n",
      "min      11.000000    11.000000    11.000000  \n",
      "25%      25.000000    25.000000    25.000000  \n",
      "50%      34.000000    34.000000    33.000000  \n",
      "75%      59.000000    58.000000    56.000000  \n",
      "max     124.000000   124.000000   124.000000  \n",
      "\n",
      "[8 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "extracted_features = []\n",
    "\n",
    "for img, label in images:\n",
    "    # if the image is not RGB, convert it\n",
    "    if len(img.shape) == 2:\n",
    "        img = si.color.gray2rgb(img)\n",
    "    \n",
    "    # remove the alpha channel\n",
    "    img = img[:, :, :3]\n",
    "\n",
    "    # binary color scale\n",
    "    img = si.color.rgb2gray(img)\n",
    "    threshold_value = si.filters.threshold_otsu(img)\n",
    "    img = img > threshold_value\n",
    "\n",
    "    # Crop the image to where the runes can be found so we can resize the rune to full size\n",
    "    img = crop_img(img)\n",
    "\n",
    "    # Resize the cropped image to 128x128\n",
    "    img = si.transform.resize(img, (124, 124)) # not 128x128 because of the padding for 2 pixels border we add later\n",
    "\n",
    "    # Check if the image is horizontally symmetric\n",
    "    horizontal_sym_errors, vertical_sym_errors = h_v_symmetry_errors(img.copy())\n",
    "\n",
    "    # Apply erosion\n",
    "    img = si.morphology.binary_erosion(img, si.morphology.square(3))\n",
    "\n",
    "    # add a 2 pixel white border around img to get correct hole count\n",
    "    img = np.pad(img, ((2, 2), (2, 2)), 'constant', constant_values=(1))\n",
    "\n",
    "    # get black pixel count for 3 x 3 sections of the img\n",
    "    black_pixels = count_black_pixels(img, 3)\n",
    "\n",
    "    # Flip colors to get the inverted image so the labeling will work on the drawn rune\n",
    "    img_inverted = np.invert(img.copy())\n",
    "\n",
    "    # Label connected components in the binary image\n",
    "    labeled_image_inverted = measure.label(img_inverted.astype(int))\n",
    "    labeled_image = measure.label(img.astype(int))\n",
    "\n",
    "    # Extract features from the labeled regions\n",
    "    props_inverted = measure.regionprops(labeled_image_inverted)\n",
    "    props = measure.regionprops(labeled_image)\n",
    "\n",
    "    # Iterate through labeled regions and extract features from the first region\n",
    "    if len(props_inverted) > 0: # if there are no regions, that means that the image has no drawing in it, we can thus ignore the else block\n",
    "        first_region = props_inverted[0]\n",
    "        features = { # all our extracted features\n",
    "            # ---- Region prop features ----\n",
    "            \"label\": label,\n",
    "            \"area\": first_region.area,\n",
    "            \"area_filled\": first_region.area_filled,\n",
    "            \"area_convex\": first_region.area_convex,\n",
    "            \"axis_major_length\": first_region.axis_major_length,\n",
    "            \"axis_minor_length\": first_region.axis_minor_length,\n",
    "            \"eccentricity\": first_region.eccentricity,\n",
    "            \"equivalent_diameter_area\": first_region.equivalent_diameter_area,\n",
    "            \"extent\": first_region.extent,\n",
    "            \"feret_diameter_max\": first_region.feret_diameter_max,\n",
    "            \"perimeter\": first_region.perimeter,\n",
    "            \"solidity\": first_region.solidity,\n",
    "            \"holes\": len(props)-1,\n",
    "            # ---- Smart features ----\n",
    "            \"h_sym_err_percent\": horizontal_sym_errors,\n",
    "            \"v_sym_err_percent\": vertical_sym_errors,\n",
    "            \"pixelsum_tl\": black_pixels[0],\n",
    "            \"pixelsum_tm\": black_pixels[1],\n",
    "            \"pixelsum_tr\": black_pixels[2],\n",
    "            \"pixelsum_ml\": black_pixels[3],\n",
    "            \"pixelsum_mm\": black_pixels[4],\n",
    "            \"pixelsum_mr\": black_pixels[5],\n",
    "            \"pixelsum_bl\": black_pixels[6],\n",
    "            \"pixelsum_bm\": black_pixels[7],\n",
    "            \"pixelsum_br\": black_pixels[8]\n",
    "        }\n",
    "        extracted_features.append((features))\n",
    "\n",
    "features_df = pd.DataFrame(extracted_features)\n",
    "\n",
    "print(features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save the data\n",
    "\n",
    "Save our selected best features to a csv file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../dataset-numpy/\" \n",
    "path = os.path.join(directory, '1.1 - features.csv')\n",
    "\n",
    "features_df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
